{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import word as word\n",
    "from importlib import reload\n",
    "reload(word)\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k 2 [('K',)]\n",
      "a 1 [('AE',)]\n",
      "e 3 [('EH',)]\n",
      "a 1 [('EY',)]\n",
      "e 3 [('IY',)]\n",
      "a 1 [('AH',), ('AO',)]\n",
      "e 3 [('IH',)]\n",
      "st- 0 [('S', 'T')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'word': 'stake',\n",
       " 'rank': 5630,\n",
       " 'part_of_speech': 'noun',\n",
       " 'is_vc': False,\n",
       " 'is_cvc': False,\n",
       " 'is_cvce': False,\n",
       " 'is_cvcvc': False,\n",
       " 'has_silent_e': True,\n",
       " 'letter_parts': ['st-', 'a', 'k', 'e'],\n",
       " 'indicators': [<Indicator.LETTER_COMBO: 5>,\n",
       "  <Indicator.LONG_VOWEL: 2>,\n",
       "  <Indicator.HARD_CONSONANT: 3>,\n",
       "  <Indicator.SILENT_E: 6>],\n",
       " 'sound_parts': [('S', 'T'), ('EY',), ('K',), ''],\n",
       " 'decodable': True,\n",
       " 'hard_consonants': array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       dtype=uint8),\n",
       " 'soft_consonants': array([0, 0], dtype=uint8),\n",
       " 'short_vowels': array([0, 0, 0, 0, 0], dtype=uint8),\n",
       " 'long_vowels': array([1, 0, 0, 0, 0], dtype=uint8),\n",
       " 'secondary_vowel_pronunciations': array([0, 0], dtype=uint8),\n",
       " 'vowel_teams': array([0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       " 'digraphs': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       " 'double_letters': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       " 'prefix_digraphs': array([0, 0, 0, 0, 0], dtype=uint8),\n",
       " 'prefix_blends': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       dtype=uint8),\n",
       " 'suffix_blends': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       " 'common_endings': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = word.Word(\"stake\")\n",
    "w.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any([False, False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1, -1,  0,  0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,1,0,0,0]) - np.array([1,0,1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['stake', 5630, 'noun', False, False, False, False, True,\n",
       "        list(['st-', 'a', 'k', 'e']),\n",
       "        list([<Indicator.LETTER_COMBO: 5>, <Indicator.LONG_VOWEL: 2>, <Indicator.HARD_CONSONANT: 3>, <Indicator.SILENT_E: 6>]),\n",
       "        list([('S', 'T'), ('EY',), ('K',), '']), True,\n",
       "        array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "              dtype=uint8)                                                 ,\n",
       "        array([0, 0], dtype=uint8), array([0, 0, 0, 0, 0], dtype=uint8),\n",
       "        array([1, 0, 0, 0, 0], dtype=uint8), array([0, 0], dtype=uint8),\n",
       "        array([0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       "        array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       "        array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       "        array([0, 0, 0, 0, 0], dtype=uint8),\n",
       "        array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "              dtype=uint8)                                                       ,\n",
       "        array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       "        array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)],\n",
       "       ['stake', 5630, 'noun', False, False, False, False, True,\n",
       "        list(['st-', 'a', 'k', 'e']),\n",
       "        list([<Indicator.LETTER_COMBO: 5>, <Indicator.LONG_VOWEL: 2>, <Indicator.HARD_CONSONANT: 3>, <Indicator.SILENT_E: 6>]),\n",
       "        list([('S', 'T'), ('EY',), ('K',), '']), True,\n",
       "        array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "              dtype=uint8)                                                 ,\n",
       "        array([0, 0], dtype=uint8), array([0, 0, 0, 0, 0], dtype=uint8),\n",
       "        array([1, 0, 0, 0, 0], dtype=uint8), array([0, 0], dtype=uint8),\n",
       "        array([0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       "        array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       "        array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       "        array([0, 0, 0, 0, 0], dtype=uint8),\n",
       "        array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "              dtype=uint8)                                                       ,\n",
       "        array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       "        array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)]], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(pd.DataFrame([w.features, w.features]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1 if p in w.decoder.letter_parts else 0 for p in utils.hard_consonants ], dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hard_consonants': array([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0], dtype=uint8),\n",
       " 'soft_consonants': array([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0], dtype=uint8),\n",
       " 'short_vowels': array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       " 'long_vowels': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       " 'secondary_vowel_pronunciations': array([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1, 1, 0, 0, 0, 0], dtype=uint8),\n",
       " 'vowel_teams': array([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0], dtype=uint8),\n",
       " 'digraphs': array([0, 0, 0, 0, 1, 0, 0, 0], dtype=uint8),\n",
       " 'double_letters': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], dtype=uint8),\n",
       " 'prefix_digraphs': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], dtype=uint8),\n",
       " 'prefix_blends': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       " 'suffix_blends': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       " 'common_endings': array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.phoneme_bitmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "for wrd in utils.simplified_cmudict:\n",
    "    w = word.Word(wrd)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /Users/cpleasants/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import word_decoder\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 8\n",
      "not 22\n",
      "but 24\n",
      "his 33\n",
      "can 38\n",
      "has 46\n",
      "had 53\n",
      "him 81\n",
      "did 113\n",
      "got 129\n",
      "man 147\n",
      "big 199\n",
      "lot 218\n",
      "god 238\n",
      "top 243\n",
      "bad 269\n",
      "job 295\n",
      "run 308\n",
      "win 407\n",
      "bit 418\n",
      "hit 420\n",
      "fun 501\n",
      "non 514\n",
      "six 518\n",
      "cut 568\n",
      "hot 710\n",
      "tax 757\n",
      "mom 788\n",
      "cup 804\n",
      "dog 825\n",
      "box 961\n",
      "gas 1050\n",
      "kid 1106\n",
      "san 1147\n",
      "sun 1153\n",
      "gun 1167\n",
      "dad 1181\n",
      "fit 1184\n",
      "fan 1269\n",
      "sit 1354\n",
      "pop 1434\n",
      "ran 1436\n",
      "fat 1481\n",
      "sad 1535\n",
      "bus 1553\n",
      "tom 1581\n",
      "mid 1607\n",
      "bag 1682\n",
      "cat 1712\n",
      "nor 1731\n",
      "van 1741\n",
      "map 1797\n",
      "fix 1825\n",
      "mad 1867\n",
      "mix 2002\n",
      "fox 2203\n",
      "max 2221\n",
      "sub 2235\n",
      "sam 2312\n",
      "sat 2313\n",
      "bob 2330\n",
      "jim 2392\n",
      "don 2479\n",
      "kim 2540\n",
      "ban 2714\n",
      "tip 2766\n",
      "hat 2834\n",
      "cap 2928\n",
      "dan 2937\n",
      "jan 2948\n",
      "tim 2970\n",
      "rid 3021\n",
      "gap 3165\n",
      "hip 3236\n",
      "mac 3312\n",
      "tag 3396\n",
      "kit 3504\n",
      "lab 3505\n",
      "min 3508\n",
      "mum 3572\n",
      "bid 3600\n",
      "hop 3628\n",
      "pot 3646\n",
      "pan 3711\n",
      "cop 3803\n",
      "log 3825\n",
      "sin 3847\n",
      "doc 3874\n",
      "rob 3978\n",
      "sum 4049\n",
      "jon 4244\n",
      "bat 4285\n",
      "con 4433\n",
      "pin 4474\n",
      "tap 4495\n",
      "pit 4547\n",
      "rip 4629\n",
      "pat 4784\n",
      "pub 4879\n",
      "bin 4919\n",
      "com 4929\n",
      "dig 4939\n",
      "ron 4977\n",
      "bug 5104\n",
      "pic 5150\n",
      "lit 5230\n",
      "ram 5253\n",
      "lap 5334\n",
      "dam 5409\n",
      "jam 5425\n",
      "rat 5445\n",
      "von 5466\n",
      "pig 5723\n",
      "rod 5730\n",
      "lip 5807\n",
      "mud 5814\n",
      "ham 5884\n",
      "rap 5906\n",
      "cam 6031\n",
      "fig 6051\n",
      "hug 6146\n",
      "bud 6217\n",
      "hub 6253\n",
      "yup 6536\n",
      "cos 6552\n",
      "dip 6560\n",
      "dot 6563\n",
      "cum 6671\n",
      "nut 6714\n",
      "mob 6809\n",
      "til 6845\n",
      "tin 6846\n",
      "gut 6914\n",
      "cal 6994\n",
      "pad 7038\n",
      "tan 7067\n",
      "pal 7257\n",
      "gig 7343\n",
      "jin 7354\n",
      "rub 7493\n",
      "tab 7619\n",
      "wit 7636\n",
      "cab 7657\n",
      "cox 7782\n",
      "gum 7798\n",
      "jun 7804\n",
      "pac 7953\n",
      "wax 8104\n",
      "mod 8171\n",
      "fog 8287\n",
      "lad 8306\n",
      "cod 8398\n",
      "hon 8435\n",
      "lil 8568\n",
      "zip 8746\n",
      "dug 8783\n",
      "rig 8849\n",
      "rim 8978\n",
      "tub 8997\n",
      "hut 9053\n",
      "mat 9064\n",
      "sox 9100\n",
      "vic 9123\n",
      "liz 9205\n",
      "nap 9368\n",
      "gal 9473\n",
      "lid 9490\n",
      "mic 9497\n",
      "lin 9622\n",
      "sim 9669\n",
      "hid 9759\n",
      "dom 9876\n",
      "bum 10338\n",
      "lib 10389\n",
      "nat 10408\n",
      "rom 10437\n",
      "mil 10578\n",
      "nod 10581\n",
      "das 10674\n",
      "dis 10678\n",
      "rug 10927\n",
      "bon 10992\n",
      "mag 11066\n",
      "mug 11071\n",
      "pod 11086\n",
      "val 11147\n",
      "fin 11209\n",
      "nam 11253\n",
      "pos 11267\n",
      "rot 11284\n",
      "dos 11369\n",
      "sol 11465\n",
      "hal 11713\n",
      "lag 11732\n",
      "sac 11788\n",
      "sip 11793\n",
      "kin 12086\n",
      "bam 12185\n",
      "dub 12227\n",
      "gus 12250\n",
      "kat 12261\n",
      "lan 12267\n",
      "nun 12288\n",
      "pup 12304\n",
      "raf 12307\n",
      "wig 12351\n",
      "fax 12418\n",
      "gag 12425\n",
      "rum 12491\n",
      "sis 12500\n",
      "dim 12577\n",
      "sic 12683\n",
      "pam 13033\n",
      "sap 13053\n",
      "tis 13072\n",
      "hum 13171\n",
      "mis 13195\n",
      "rib 13229\n",
      "hog 13531\n",
      "sid 13609\n",
      "tor 13624\n",
      "bun 13673\n",
      "din 13697\n",
      "sup 13817\n",
      "dat 13897\n",
      "gil 13922\n",
      "mal 13952\n",
      "pun 13992\n",
      "rag 13995\n",
      "lax 14156\n",
      "vat 14245\n",
      "tad 14432\n",
      "yan 14457\n",
      "cas 14712\n",
      "por 15028\n",
      "rad 15038\n",
      "tug 15078\n",
      "cub 15139\n",
      "nan 15225\n",
      "sal 15497\n",
      "tit 15532\n",
      "sas 15696\n",
      "yum 15943\n",
      "lam 16265\n",
      "mos 16535\n",
      "pol 16560\n",
      "vis 16613\n",
      "hoc 16727\n",
      "vin 16823\n",
      "cad 16857\n",
      "maj 16962\n",
      "pak 17198\n",
      "fab 17362\n",
      "liv 17401\n",
      "mop 17421\n",
      "hun 17633\n",
      "mir 17675\n",
      "kan 17878\n",
      "bop 18027\n",
      "hud 18338\n",
      "jab 18351\n",
      "pip 18395\n",
      "tat 18445\n",
      "yin 18464\n",
      "yun 18465\n",
      "bis 18496\n",
      "bog 18502\n",
      "bom 18503\n",
      "dun 18809\n",
      "lim 18873\n",
      "lux 18877\n",
      "nil 18904\n",
      "cor 19047\n",
      "dab 19055\n",
      "fag 19082\n",
      "fil 19084\n",
      "nip 19148\n",
      "lac 19378\n",
      "roc 19438\n",
      "jog 19625\n",
      "jug 19627\n",
      "pax 19654\n",
      "ros 19684\n",
      "pap 20177\n",
      "tac 20242\n",
      "tam 20244\n",
      "wil 20267\n",
      "biz 20297\n",
      "kun 20403\n",
      "sig 20496\n",
      "sob 20502\n",
      "fad 20633\n",
      "nav 20715\n",
      "dal 20896\n",
      "sag 21051\n",
      "sod 21071\n",
      "tak 21377\n",
      "wis 21425\n",
      "vox 21698\n",
      "nom 21879\n",
      "cot 22044\n",
      "ras 22192\n",
      "tic 22243\n",
      "som 22818\n",
      "nag 23046\n",
      "mor 23336\n",
      "pom 23362\n",
      "dum 23516\n",
      "mol 23601\n",
      "tod 23705\n",
      "tot 23706\n",
      "coz 23791\n",
      "lon 23892\n",
      "sax 23981\n",
      "dak 24133\n",
      "ric 24298\n",
      "bal 24385\n",
      "hof 24472\n",
      "mak 24502\n",
      "wiz 24638\n",
      "nab 24838\n",
      "yak 24961\n",
      "lok 25124\n",
      "mig 25136\n",
      "pug 25179\n",
      "rut 25193\n",
      "fob 25387\n",
      "jig 25438\n",
      "lis 25821\n",
      "lug 25825\n",
      "nit 25858\n",
      "dax 26054\n",
      "lal 26143\n",
      "rus 26233\n",
      "bos 26349\n",
      "gan 26438\n",
      "dix 26759\n",
      "kal 26852\n",
      "kam 26853\n",
      "nib 26900\n",
      "zig 27347\n",
      "cob 27403\n",
      "hap 27485\n",
      "kip 27529\n",
      "kos 27533\n",
      "nik 27582\n",
      "sus 27666\n",
      "yam 27703\n",
      "luz 27907\n",
      "wag 28080\n",
      "cus 28162\n",
      "pus 28335\n",
      "suk 28392\n",
      "zak 28436\n",
      "cog 28495\n",
      "mun 28630\n",
      "pox 28673\n",
      "bab 28824\n",
      "nix 29012\n",
      "dud 29250\n",
      "jap 29325\n",
      "sil 29460\n",
      "lob 29721\n",
      "bib 29932\n",
      "cul 29985\n",
      "hag 30074\n",
      "bok 30338\n",
      "dac 30388\n",
      "kok 30476\n",
      "zap 30661\n",
      "tut 31383\n",
      "yap 31417\n",
      "bol 31877\n",
      "gad 32003\n",
      "nad 32510\n",
      "bak 32687\n",
      "vos 33076\n",
      "dag 33193\n",
      "dol 33208\n",
      "baz 33532\n",
      "bub 33546\n",
      "sul 33867\n",
      "cac 33968\n",
      "lor 34164\n",
      "sop 34292\n",
      "wop 34352\n",
      "jot 34553\n",
      "yom 34755\n",
      "yon 34756\n",
      "gul 34904\n",
      "viv 35150\n",
      "fac 35713\n",
      "gab 35727\n",
      "hom 35754\n",
      "tux 35995\n",
      "wor 36015\n",
      "gat 36153\n",
      "yip 36405\n",
      "pon 36712\n",
      "rab 36722\n",
      "duc 36965\n",
      "jib 37526\n",
      "sor 37699\n",
      "wok 37772\n",
      "zag 37779\n",
      "bil 37825\n",
      "mim 38050\n",
      "tig 38205\n",
      "bic 38307\n",
      "rog 38597\n",
      "hob 38888\n",
      "pil 39026\n",
      "pix 39482\n",
      "mib 39911\n",
      "fis 40244\n",
      "jac 40302\n",
      "sot 40452\n",
      "vax 40502\n",
      "zim 40528\n",
      "kon 40769\n",
      "tol 41432\n",
      "yuk 41483\n",
      "dif 41593\n",
      "gob 41646\n",
      "lop 41740\n",
      "vim 42479\n",
      "rox 42871\n",
      "dic 43108\n",
      "lun 43243\n",
      "nir 43809\n",
      "dob 44169\n",
      "lox 44311\n",
      "vaz 44557\n",
      "dov 44747\n",
      "luk 45494\n",
      "kot 46576\n",
      "mab 47174\n",
      "lum 47735\n",
      "gib 48253\n",
      "roz 48461\n",
      "yul 49734\n",
      "nub 50117\n",
      "jut 50618\n",
      "bip 50980\n",
      "zan 53963\n",
      "mok 54342\n",
      "suc 55778\n",
      "bax 55943\n",
      "lak 56236\n",
      "rak 56392\n",
      "vig 56518\n",
      "fop 56749\n",
      "raz 57007\n",
      "ruf 57035\n",
      "duk 57989\n",
      "gaf 58041\n",
      "pog 59632\n",
      "sib 60470\n",
      "vik 61202\n",
      "pik 61706\n",
      "riz 63952\n",
      "sak 63967\n",
      "dux 65078\n",
      "kis 65237\n",
      "wix 65639\n",
      "kut 66024\n",
      "sok 67844\n",
      "yim 67971\n",
      "rix 69398\n",
      "bix 69687\n",
      "mof 70089\n",
      "kir 71632\n",
      "zug 72911\n",
      "faz 73174\n",
      "jil 73319\n",
      "kuk 73359\n",
      "rud 74467\n",
      "buc 81187\n",
      "hix 82384\n",
      "zon 84936\n",
      "jif 85392\n",
      "hok 88426\n",
      "hux 93808\n",
      "kus 96144\n",
      "ziv 97740\n"
     ]
    }
   ],
   "source": [
    "import word_decoder\n",
    "import utils\n",
    "decoded_dict = {}\n",
    "for word in utils.simplified_cmudict:\n",
    "    decoder = word_decoder.WordDecoder(word)\n",
    "    decoded_dict[word] = decoder.decoded\n",
    "    if decoder.is_cvc():\n",
    "        print(word, decoder.word_rank)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'letter_parts': ['c', 'a', 't'],\n",
       " 'indicators': [<Indicator.HARD_CONSONANT: 3>,\n",
       "  <Indicator.SHORT_VOWEL: 1>,\n",
       "  <Indicator.HARD_CONSONANT: 3>],\n",
       " 'sound_parts': [('K',), ('AE',), ('T',)],\n",
       " 'decodable': True}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_dict['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phonemes import *\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "from wordfreq import top_n_list\n",
    "from collections.abc import Iterable\n",
    "import re\n",
    "from enum import Enum, auto\n",
    "\n",
    "TOP_N = 100000\n",
    "VOWELS = {'a', 'e', 'i', 'o', 'u'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /Users/cpleasants/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('cmudict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_emphasis(word_phonemes: list):\n",
    "    stripped = []\n",
    "    for phoneme in word_phonemes:\n",
    "        stripped.append(''.join(c for c in phoneme if c.isalpha()))\n",
    "    return stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = top_n_list('en', TOP_N)\n",
    "full_cmudict = cmudict.dict()\n",
    "\n",
    "simplified_cmudict = {\n",
    "    word : strip_emphasis(full_cmudict[word][0]) for word in top_n if word in full_cmudict\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flat_list(li:Iterable):\n",
    "    \"\"\"\n",
    "    Flattens an iterable into a single list.\n",
    "\n",
    "    If an element in the input iterable is a list, set, or tuple, its items are\n",
    "    added individually to the output list. Strings and other non-iterables are\n",
    "    treated as single elements.\n",
    "\n",
    "    Args:\n",
    "        li (Iterable): The input iterable, which may contain nested lists, sets, or tuples.\n",
    "\n",
    "    Returns:\n",
    "        list: A flattened list containing all individual elements.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    for value in li:\n",
    "        if isinstance(value, Iterable) and not isinstance(value, str):\n",
    "            output.extend(value)\n",
    "        else:\n",
    "            output.append(value)\n",
    "    return output\n",
    "\n",
    "# Create a list of short vowel sounds, long vowel sounds, and any vowel sound:\n",
    "short_vowel_sounds = get_flat_list(short_vowels.values())\n",
    "long_vowels_sounds = get_flat_list(long_vowels.values())\n",
    "vowel_team_sounds = get_flat_list(vowel_teams.values())\n",
    "all_vowel_sounds = set(short_vowel_sounds + long_vowels_sounds + vowel_team_sounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_combinations = vowel_teams | digraphs | double_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bl-': [('B', 'L')], 'cl-': [('K', 'L')], 'fl-': [('F', 'L')], 'gl-': [('G', 'L')], 'pl-': [('P', 'L')], 'sl-': [('S', 'L')], 'br-': [('B', 'R')], 'cr-': [('K', 'R')], 'dr-': [('D', 'R')], 'fr-': [('F', 'R')], 'gr-': [('G', 'R')], 'pr-': [('P', 'R')], 'tr-': [('T', 'R')], 'sc-': [('S', 'C')], 'shr-': [('SH', 'R')], 'sk-': [('S', 'K')], 'sm-': [('S', 'M')], 'sn-': [('S', 'N')], 'sp-': [('S', 'P')], 'squ-': [('S', 'K', 'W')], 'st-': [('S', 'T')], 'sw-': [('S', 'W')]}\n",
      "{'-lp': [('L', 'P')], '-st': [('S', 'T')], '-ct': [('K', 'T')], '-pt': [('P', 'T')], '-sk': [('S', 'K')], '-lk': [('K',)], '-lf': [('L', 'F')], '-xt': [('K', 'S', 'T')], '-ft': [('F', 'T')], '-nd': [('N', 'D')], '-mp': [('M', 'P')], '-lt': [('L', 'T')], '-nch': [('N', 'CH')], '-mb': [('M', 'B')], '-tch': [('CH',)], '-dge': [('JH',)], '-ing': [('IH', 'NG')], '-ang': [('AE', 'NG')], '-ong': [('AO', 'NG')], '-ung': [('AH', 'NG')], '-ank': [('AE', 'NG', 'K')], '-ink': [('IH', 'NG', 'K')], '-onk': [('AA', 'NG', 'K')], '-unk': [('AH', 'NG', 'K')], '-oe': [('OW',)], '-ed': [('EH', 'D'), 'D']}\n"
     ]
    }
   ],
   "source": [
    "def is_prefix(blend_or_digraph:str):\n",
    "    return blend_or_digraph[-1] == '-'\n",
    "\n",
    "def is_suffix(blend_or_digraph:str):\n",
    "    return blend_or_digraph[0] == '-'\n",
    "\n",
    "prefixes = dict()\n",
    "suffixes = dict()\n",
    "\n",
    "for d in [vowel_teams, digraphs, prefix_blends, suffix_blends, common_endings]:\n",
    "    prefixes = prefixes | {k: v for k, v in d.items() if is_prefix(k)}\n",
    "    suffixes = suffixes | {k: v for k, v in d.items() if is_suffix(k)}\n",
    "\n",
    "print(prefixes)\n",
    "print(suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AH', 'K', 'S', 'EH', 'N', 'T']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_cmudict['accent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indicator(Enum):\n",
    "    SHORT_VOWEL = auto()\n",
    "    LONG_VOWEL = auto()\n",
    "    HARD_CONSONANT = auto()\n",
    "    SOFT_CONSONANT = auto()\n",
    "    LETTER_COMBO = auto()\n",
    "    SILENT_E = auto()\n",
    "    UNDECODABLE = auto()\n",
    "    \n",
    "def decode(word:str):\n",
    "    \"\"\"\n",
    "    Decodes a word into its phonetic components using a simplified CMU dictionary.\n",
    "    \n",
    "    The function identifies prefixes, suffixes, and letter-to-sound mappings to determine\n",
    "    how the word is structured phonetically.\n",
    "\n",
    "    Args:\n",
    "        word (str): The word to decode.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - 'letter_parts': List of identified letter components.\n",
    "            - 'indicators': List of Indicator values\n",
    "            - 'sound_parts': Corresponding phonetic components.\n",
    "            - 'decodable': Boolean indicating if the word is fully decodable.\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    if word not in simplified_cmudict:\n",
    "        raise Exception(\"Word not found\")\n",
    "    \n",
    "    word_phonemes = simplified_cmudict[word]\n",
    "    remaining_letters = word\n",
    "    remaining_sounds = word_phonemes\n",
    "    letter_parts = []\n",
    "    sound_parts = []\n",
    "    indicators = []\n",
    "    decodable = True\n",
    "\n",
    "    # ---- Helper Functions ----\n",
    "\n",
    "    def handle_undecodable():\n",
    "        \"\"\"Handles cases where the letter-sound mapping isn't recognized.\"\"\"\n",
    "        nonlocal decodable, remaining_letters, remaining_sounds\n",
    "        decodable = False\n",
    "        letter_parts.append(remaining_letters)\n",
    "        sound_parts.append(tuple(remaining_sounds))\n",
    "        indicators.append(Indicator.UNDECODABLE)\n",
    "        remaining_letters = ''\n",
    "        remaining_sounds = []\n",
    "\n",
    "    def process_single_letter_sound(letter, sound, indicator):\n",
    "        \"\"\"Handles normal, single-letter-sound processing.\"\"\"\n",
    "        nonlocal remaining_letters, remaining_sounds\n",
    "        letter_parts.append(letter)\n",
    "        sound_parts.append(sound)\n",
    "        indicators.append(indicator)\n",
    "        remaining_letters = remaining_letters[1:]\n",
    "        remaining_sounds = remaining_sounds[len(sound) : ] if remaining_sounds else []\n",
    "\n",
    "    # Parse out any prefixes and suffixes\n",
    "    def process_affixes(affixes_dict, is_prefix = True):\n",
    "        \"\"\"Handles both prefixes and suffixes processing.\"\"\"\n",
    "        nonlocal remaining_letters, remaining_sounds\n",
    "        affix_letter_parts = []\n",
    "        affix_sound_parts = []\n",
    "        affix_indicators = []\n",
    "\n",
    "        for affix, affix_sounds in affixes_dict.items():\n",
    "            affix_letters = affix.replace('-', '')\n",
    "            letters_match = word.startswith(affix_letters) if is_prefix else word.endswith(affix_letters)\n",
    "            for affix_sound in affix_sounds:\n",
    "                sounds_match = tuple(word_phonemes[ : len(affix_sound)]) == affix_sound if is_prefix else tuple(word_phonemes[-len(affix_sound) : ]) == affix_sound\n",
    "                num_sounds = len(affix_sound)\n",
    "                if letters_match and sounds_match:\n",
    "                    affix_letter_parts.append(affix)\n",
    "                    affix_sound_parts.append(affix_sound)\n",
    "                    affix_indicators.append(Indicator.LETTER_COMBO)\n",
    "                    remaining_letters = remaining_letters.lstrip(affix_letters) if is_prefix else remaining_letters.rstrip(affix_letters)\n",
    "                    remaining_sounds = remaining_sounds[num_sounds:] if is_prefix else remaining_sounds[ : -num_sounds]\n",
    "                    break\n",
    "        return affix_letter_parts, affix_sound_parts, affix_indicators\n",
    "    \n",
    "\n",
    "    # ---- Main Logic ----\n",
    "    \n",
    "    # Process prefixes and suffixes\n",
    "    prefix_letter_parts, prefix_sound_parts, prefix_indicators = process_affixes(prefixes)\n",
    "    suffix_letters, suffix_sound_parts, suffix_indicators = process_affixes(suffixes, is_prefix = False)\n",
    "\n",
    "    \n",
    "\n",
    "    # Process through the remaining_letters:\n",
    "    while len(remaining_letters) > 0:\n",
    "        # first search through all letter combinations\n",
    "        for letters, sounds in letter_combinations.items():\n",
    "            for sound in sounds:\n",
    "                if remaining_letters.startswith(letters) and tuple(remaining_sounds[:len(sound)]) == sound:\n",
    "                    letter_parts.append(letters)\n",
    "                    sound_parts.append(sound)\n",
    "                    indicators.append(Indicator.LETTER_COMBO)\n",
    "                    remaining_letters = remaining_letters.lstrip(letters)\n",
    "                    remaining_sounds = remaining_sounds[len(sound) : ]\n",
    "                    break\n",
    "            else:\n",
    "                # Continue searching other letter combinations if no match is found\n",
    "                continue\n",
    "            # If the inner break is hit, break the outer loop as well\n",
    "            break\n",
    "        # Some words contain punctuation (e.g. \"won't\") -- skip the punctuation\n",
    "        else:\n",
    "            this_letter = remaining_letters[0]\n",
    "            if not this_letter.isalpha():\n",
    "                remaining_letters = remaining_letters[1:]\n",
    "                continue\n",
    "\n",
    "            matched = False\n",
    "\n",
    "            # Silent E\n",
    "            if (len(remaining_sounds) == 0 or remaining_sounds[0] not in all_vowel_sounds) and this_letter == 'e':\n",
    "                process_single_letter_sound(this_letter, '', Indicator.SILENT_E)\n",
    "                matched = True\n",
    "            \n",
    "            for indicator, sound_dict in SOUND_CATEGORIES.items():\n",
    "                for sound in sound_dict.get(this_letter, []):\n",
    "                    if tuple(remaining_sounds[:len(sound)]) == sound:\n",
    "                        process_single_letter_sound(this_letter, sound, indicator)\n",
    "                        matched = True\n",
    "                        break\n",
    "                if matched:\n",
    "                    break\n",
    "                \n",
    "            if not matched:\n",
    "                handle_undecodable()\n",
    "            \n",
    "\n",
    "    # Add back in the suffixes\n",
    "    letter_parts = prefix_letter_parts + letter_parts + suffix_letters\n",
    "    sound_parts = prefix_sound_parts + sound_parts + suffix_sound_parts\n",
    "    indicators = prefix_indicators + indicators + suffix_indicators\n",
    "\n",
    "    return {\n",
    "        'letter_parts' : letter_parts, \n",
    "        'indicators' : indicators ,\n",
    "        'sound_parts' : sound_parts,\n",
    "        'decodable' : decodable\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'letter_parts': ['l', 'a', 'k', 'e', 's'],\n",
       " 'indicators': [<Indicator.HARD_CONSONANT: 3>,\n",
       "  <Indicator.LONG_VOWEL: 2>,\n",
       "  <Indicator.HARD_CONSONANT: 3>,\n",
       "  <Indicator.SILENT_E: 6>,\n",
       "  <Indicator.HARD_CONSONANT: 3>],\n",
       " 'sound_parts': [('L',), ('EY',), ('K',), '', ('S',)],\n",
       " 'decodable': True}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode('lakes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'remaining_letters' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m utils\u001b[38;5;241m.\u001b[39msimplified_cmudict:\n\u001b[1;32m      3\u001b[0m     decoder \u001b[38;5;241m=\u001b[39m word_decoder\u001b[38;5;241m.\u001b[39mWordDecoder(word)\n\u001b[0;32m----> 4\u001b[0m     decoded_dict[word] \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode()\n",
      "File \u001b[0;32m~/decodable-word-generator/word_decoder.py:86\u001b[0m, in \u001b[0;36mWordDecoder.decode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m letters, sounds \u001b[38;5;129;01min\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mletter_combinations\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sound \u001b[38;5;129;01min\u001b[39;00m sounds:\n\u001b[0;32m---> 86\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining_letters\u001b[38;5;241m.\u001b[39mstartswith(letters) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(remaining_sounds[:\u001b[38;5;28mlen\u001b[39m(sound)]) \u001b[38;5;241m==\u001b[39m sound:\n\u001b[1;32m     87\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mletter_parts\u001b[38;5;241m.\u001b[39mappend(letters)\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msound_parts\u001b[38;5;241m.\u001b[39mappend(sound)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'remaining_letters' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "decoded_dict = {}\n",
    "for word in utils.simplified_cmudict:\n",
    "    decoder = word_decoder.WordDecoder(word)\n",
    "    decoded_dict[word] = decoder.decode()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meanness\n",
      "{'letter_parts': ['m', 'ea', 'nn', 'e', 'ss'], 'indicators': [<Indicator.HARD_CONSONANT: 3>, <Indicator.LETTER_COMBO: 5>, <Indicator.LETTER_COMBO: 5>, <Indicator.SILENT_E: 6>, <Indicator.UNDECODABLE: 7>], 'sound_parts': [('M',), ('IY',), ('N',), '', ('N', 'AH', 'S')], 'decodable': False}\n",
      "manically\n",
      "\n",
      "milfs\n",
      "\n",
      "statistically\n",
      "{'letter_parts': ['st-', 'atistically'], 'indicators': [<Indicator.LETTER_COMBO: 5>, <Indicator.UNDECODABLE: 7>], 'sound_parts': [('S', 'T'), ('AH', 'T', 'IH', 'S', 'T', 'IH', 'K', 'AH', 'L', 'IY')], 'decodable': False}\n",
      "maughan\n",
      "{'letter_parts': ['m', 'aughan'], 'indicators': [<Indicator.HARD_CONSONANT: 3>, <Indicator.UNDECODABLE: 7>], 'sound_parts': [('M',), ('AO', 'G', 'AH', 'N')], 'decodable': False}\n",
      "hollowed\n",
      "{'letter_parts': ['h', 'o', 'll', 'ow', 'e', 'd'], 'indicators': [<Indicator.HARD_CONSONANT: 3>, <Indicator.SHORT_VOWEL: 1>, <Indicator.LETTER_COMBO: 5>, <Indicator.LETTER_COMBO: 5>, <Indicator.SILENT_E: 6>, <Indicator.HARD_CONSONANT: 3>], 'sound_parts': [('HH',), ('AA',), ('L',), ('OW',), '', ('D',)], 'decodable': True}\n",
      "greek\n",
      "{'letter_parts': ['gr-', 'ee', 'k'], 'indicators': [<Indicator.LETTER_COMBO: 5>, <Indicator.LETTER_COMBO: 5>, <Indicator.HARD_CONSONANT: 3>], 'sound_parts': [('G', 'R'), ('IY',), ('K',)], 'decodable': True}\n",
      "laurentian\n",
      "{'letter_parts': ['l', 'aurentian'], 'indicators': [<Indicator.HARD_CONSONANT: 3>, <Indicator.UNDECODABLE: 7>], 'sound_parts': [('L',), ('AO', 'R', 'EH', 'N', 'SH', 'AH', 'N')], 'decodable': False}\n",
      "poplar\n",
      "{'letter_parts': ['p', 'o', 'p', 'l', 'ar'], 'indicators': [<Indicator.HARD_CONSONANT: 3>, <Indicator.SHORT_VOWEL: 1>, <Indicator.HARD_CONSONANT: 3>, <Indicator.HARD_CONSONANT: 3>, <Indicator.UNDECODABLE: 7>], 'sound_parts': [('P',), ('AA',), ('P',), ('L',), ('ER',)], 'decodable': False}\n",
      "beauteous\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    ind = random.randint(0, len(top_n))\n",
    "    print(top_n[ind])\n",
    "    print(decode(top_n[ind])) if top_n[ind] in simplified_cmudict else print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P', 'EH', 'S', 'T', 'ER', 'D']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_cmudict['pestered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'123'[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anket ['AE', 'NG', 'K', 'AH', 'T']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'letter_parts': ['bl-'], 'sound_parts': [('B', 'L')]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode('blanket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = [('bl', ('B', 'L')), '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: There is a challenge with \"qu\" diagraphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B', 'UH', 'K']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_cmudict[\"book\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "would\n",
      "could\n",
      "should\n",
      "you're\n",
      "during\n",
      "sure\n",
      "put\n",
      "full\n",
      "woman\n",
      "security\n",
      "wouldn't\n",
      "couldn't\n",
      "european\n",
      "europe\n",
      "tour\n",
      "insurance\n",
      "fully\n",
      "pull\n",
      "push\n",
      "shouldn't\n",
      "yours\n",
      "ensure\n",
      "sugar\n",
      "pulled\n",
      "schedule\n",
      "pure\n",
      "secure\n",
      "rural\n",
      "tournament\n",
      "surely\n",
      "puts\n",
      "bush\n",
      "jury\n",
      "bullshit\n",
      "curious\n",
      "scheduled\n",
      "pushing\n",
      "output\n",
      "pushed\n",
      "bureau\n",
      "would've\n",
      "input\n",
      "pulling\n",
      "wolf\n",
      "bull\n",
      "cure\n",
      "mature\n",
      "tourism\n",
      "missouri\n",
      "bullet\n",
      "pussy\n",
      "securities\n",
      "secured\n",
      "tourist\n",
      "woman's\n",
      "duration\n",
      "tours\n",
      "handful\n",
      "jurisdiction\n",
      "purely\n",
      "should've\n",
      "assured\n",
      "assure\n",
      "could've\n",
      "tourists\n",
      "bullets\n",
      "pulls\n",
      "wolves\n",
      "ensuring\n",
      "furious\n",
      "bulls\n",
      "bullying\n",
      "curiosity\n",
      "superb\n",
      "securing\n",
      "fury\n",
      "touring\n",
      "bully\n",
      "europeans\n",
      "maturity\n",
      "muhammad\n",
      "fulfill\n",
      "yourselves\n",
      "obscure\n",
      "endure\n",
      "assurance\n",
      "schedules\n",
      "bulletin\n",
      "fuller\n",
      "jung\n",
      "fulfilled\n",
      "neural\n",
      "pushes\n",
      "butcher\n",
      "unsure\n",
      "cured\n",
      "miniature\n",
      "fulfilling\n",
      "neurons\n",
      "boulevard\n",
      "premature\n",
      "bullied\n",
      "insured\n",
      "purity\n",
      "scheduling\n",
      "durable\n",
      "procurement\n",
      "enduring\n",
      "lure\n",
      "immature\n",
      "samurai\n",
      "bulldogs\n",
      "inputs\n",
      "yuri\n",
      "pudding\n",
      "ensures\n",
      "europa\n",
      "honduras\n",
      "jurisdictions\n",
      "worcester\n",
      "zurich\n",
      "bushes\n",
      "cushion\n",
      "endured\n",
      "insecurity\n",
      "ambush\n",
      "blvd\n",
      "bureaucracy\n",
      "curator\n",
      "plural\n",
      "ensured\n",
      "neurological\n",
      "insurer\n",
      "europe's\n",
      "fulton\n",
      "jurassic\n",
      "pulmonary\n",
      "durant\n",
      "insurers\n",
      "mural\n",
      "fullest\n",
      "schultz\n",
      "bullies\n",
      "jurors\n",
      "octopus\n",
      "neuroscience\n",
      "fulfillment\n",
      "wolverine\n",
      "lumpur\n",
      "bullock\n",
      "sugars\n",
      "wolfe\n",
      "bourgeois\n",
      "bureaucratic\n",
      "insure\n",
      "kabul\n",
      "reassuring\n",
      "toured\n",
      "outputs\n",
      "bulldog\n",
      "brochure\n",
      "butch\n",
      "contour\n",
      "fluorescent\n",
      "matured\n",
      "gourmet\n",
      "gupta\n",
      "pulitzer\n",
      "cures\n",
      "junta\n",
      "bullpen\n",
      "werewolf\n",
      "assurances\n",
      "couture\n",
      "eureka\n",
      "turnbull\n",
      "muir\n",
      "prematurely\n",
      "sakura\n",
      "turin\n",
      "busch\n",
      "reassure\n",
      "purification\n",
      "insecurities\n",
      "securely\n",
      "bush's\n",
      "reassurance\n",
      "curing\n",
      "lured\n",
      "wolfgang\n",
      "fluoride\n",
      "uneducated\n",
      "assures\n",
      "neurology\n",
      "pulpit\n",
      "pussies\n",
      "uninsured\n",
      "wolff\n",
      "fullback\n",
      "obscured\n",
      "ambushed\n",
      "purest\n",
      "curiously\n",
      "detour\n",
      "manure\n",
      "obscurity\n",
      "cushions\n",
      "entourage\n",
      "furiously\n",
      "purified\n",
      "turing\n",
      "bullion\n",
      "bureaucrats\n",
      "gustav\n",
      "throughput\n",
      "telugu\n",
      "procure\n",
      "allure\n",
      "unsecured\n",
      "bullish\n",
      "jurisprudence\n",
      "murals\n",
      "juries\n",
      "juror\n",
      "sugary\n",
      "assuring\n",
      "maturing\n",
      "secures\n",
      "spokeswoman\n",
      "contours\n",
      "reassured\n",
      "arturo\n",
      "pullman\n",
      "brochures\n",
      "neurotic\n",
      "procured\n",
      "schulz\n",
      "worcestershire\n",
      "butchers\n",
      "fourier\n",
      "muriel\n",
      "impurities\n",
      "neuro\n",
      "nuremberg\n",
      "purify\n",
      "bosom\n",
      "centurion\n",
      "curators\n",
      "nakamura\n",
      "bulletins\n",
      "neurologist\n",
      "bourgeoisie\n",
      "curate\n",
      "fullness\n",
      "incurable\n",
      "puss\n",
      "alluring\n",
      "duress\n",
      "infuriating\n",
      "butchered\n",
      "eurasia\n",
      "kush\n",
      "mouthful\n",
      "puritan\n",
      "fullerton\n",
      "pushy\n",
      "eurasian\n",
      "bull's\n",
      "bushy\n",
      "endures\n",
      "figuratively\n",
      "cushing\n",
      "infuriated\n",
      "posthumous\n",
      "spurious\n",
      "sugarcane\n",
      "businesswoman\n",
      "impure\n",
      "bureaus\n",
      "hurrah\n",
      "lures\n",
      "milieu\n",
      "punta\n",
      "pusher\n",
      "unfulfilled\n",
      "beowulf\n",
      "buren\n",
      "congresswoman\n",
      "fulfills\n",
      "manchuria\n",
      "matures\n",
      "procuring\n",
      "pulley\n",
      "pluralism\n",
      "chairwoman\n",
      "bulldozer\n",
      "curie\n",
      "fleury\n",
      "luring\n",
      "posthumously\n",
      "puree\n",
      "gilmour\n",
      "honduran\n",
      "spatula\n",
      "fulbright\n",
      "fulltime\n",
      "overtures\n",
      "bureaucrat\n",
      "jury's\n",
      "neurosurgery\n",
      "nur\n",
      "wolf's\n",
      "acapulco\n",
      "durand\n",
      "neuropathy\n",
      "butcher's\n",
      "insuring\n",
      "mercurial\n",
      "puri\n",
      "reinsurance\n",
      "surety\n",
      "catwoman\n",
      "immaturity\n",
      "muhammed\n",
      "neurosurgeon\n",
      "politburo\n",
      "bushel\n",
      "nomura\n",
      "superlative\n",
      "assuredly\n",
      "bullard\n",
      "jurisdictional\n",
      "purifying\n",
      "puritans\n",
      "wolfsburg\n",
      "artur\n",
      "bonjour\n",
      "bulwark\n",
      "fulcrum\n",
      "juris\n",
      "polyurethane\n",
      "curiosities\n",
      "jurists\n",
      "obscuring\n",
      "pussycat\n",
      "sulfuric\n",
      "murakami\n",
      "vulva\n",
      "jure\n",
      "kimura\n",
      "lurid\n",
      "sunnis\n",
      "tour's\n",
      "bureau's\n",
      "gurion\n",
      "heuristic\n",
      "pushkin\n",
      "tournament's\n",
      "cour\n",
      "durations\n",
      "kibbutz\n",
      "puddings\n",
      "wolfram\n",
      "fluorine\n",
      "immanuel\n",
      "jurist\n",
      "neuroscientist\n",
      "pulsar\n",
      "dura\n",
      "impurity\n",
      "platypus\n",
      "wolfson\n",
      "intramural\n",
      "pushover\n",
      "pullin\n",
      "boulevards\n",
      "curable\n",
      "curative\n",
      "cushy\n",
      "purebred\n",
      "purists\n",
      "bushnell\n",
      "scheduler\n",
      "dachshund\n",
      "detours\n",
      "murad\n",
      "bulldozers\n",
      "butchering\n",
      "cushman\n",
      "segundo\n",
      "turismo\n",
      "cushioned\n",
      "flatbush\n",
      "obscures\n",
      "puller\n",
      "qureshi\n",
      "sakurai\n",
      "surest\n",
      "pulleys\n",
      "ural\n",
      "womanizer\n",
      "bushels\n",
      "durante\n",
      "handfuls\n",
      "pullout\n",
      "purist\n",
      "ruhr\n",
      "injurious\n",
      "jura\n",
      "murano\n",
      "purifier\n",
      "shultz\n",
      "beaulieu\n",
      "buller\n",
      "latour\n",
      "neurosis\n",
      "pluralistic\n",
      "pushers\n",
      "reassures\n",
      "butchery\n",
      "demure\n",
      "neurologic\n",
      "puritanical\n",
      "pushups\n",
      "sura\n",
      "bureaucracies\n",
      "bushings\n",
      "courant\n",
      "murine\n",
      "pura\n",
      "ambushes\n",
      "fullbacks\n",
      "policewoman\n",
      "tourmaline\n",
      "venturi\n",
      "insurances\n",
      "missouri's\n",
      "miura\n",
      "purer\n",
      "urals\n",
      "curatorial\n",
      "furor\n",
      "heuristics\n",
      "muhammad's\n",
      "pullback\n",
      "rosamund\n",
      "shulman\n",
      "curitiba\n",
      "insures\n",
      "tobruk\n",
      "arthurian\n",
      "kurosawa\n",
      "cardiopulmonary\n",
      "councilwoman\n",
      "kirkuk\n",
      "neurologists\n",
      "segura\n",
      "bushman\n",
      "infuriates\n",
      "jourdan\n",
      "security's\n",
      "superlatives\n",
      "bogie\n",
      "bullfighting\n",
      "eurostar\n",
      "murata\n",
      "wolfman\n",
      "fuhrer\n",
      "purim\n",
      "lurie\n",
      "purina\n",
      "shure\n",
      "contoured\n",
      "curio\n",
      "cushioning\n",
      "yurt\n",
      "bulldozed\n",
      "fluoridation\n",
      "muirhead\n",
      "curacao\n",
      "infuriate\n",
      "dufour\n",
      "wulf\n",
      "maturities\n",
      "potpourri\n",
      "wintour\n",
      "compulsions\n",
      "cruickshank\n",
      "nishimura\n",
      "schulze\n",
      "fuller's\n",
      "jung's\n",
      "mura\n",
      "ture\n",
      "putsch\n",
      "tamura\n",
      "bullfrog\n",
      "duro\n",
      "neuroses\n",
      "purifiers\n",
      "fluorite\n",
      "securitization\n",
      "bourque\n",
      "bulldoze\n",
      "murrieta\n",
      "bullhorn\n",
      "cuervo\n",
      "curios\n",
      "guenther\n",
      "bullocks\n",
      "couturier\n",
      "epicurean\n",
      "irkutsk\n",
      "superwoman\n",
      "ambushing\n",
      "furtado\n",
      "mercure\n",
      "pullen\n",
      "bullen\n",
      "janusz\n",
      "moura\n",
      "puritanism\n",
      "saleswoman\n",
      "sugared\n",
      "bravura\n",
      "fulford\n",
      "muirfield\n",
      "wulff\n",
      "fultz\n",
      "neurosurgeons\n",
      "steppenwolf\n",
      "tush\n",
      "bulla\n",
      "bullfighter\n",
      "eurocopter\n",
      "surinam\n",
      "wolfe's\n",
      "womanizing\n",
      "bundesbank\n",
      "bushmen\n",
      "englishwoman\n",
      "plurals\n",
      "yury\n",
      "bullock's\n",
      "bullwinkle\n",
      "duracell\n",
      "inured\n",
      "trueman\n",
      "kumbaya\n",
      "luria\n",
      "bullfight\n",
      "pulver\n",
      "flournoy\n",
      "surer\n",
      "wolverton\n",
      "aneurism\n",
      "prurient\n",
      "sugar's\n",
      "sugarman\n",
      "bourget\n",
      "tellurium\n",
      "bulldozing\n",
      "bullhead\n",
      "datura\n",
      "fulsome\n",
      "pulpits\n",
      "assemblywoman\n",
      "duryea\n",
      "neurofibromatosis\n",
      "sihanouk\n",
      "fuhrman\n",
      "zuri\n",
      "niebuhr\n",
      "pussycats\n",
      "wolverine's\n",
      "arcturus\n",
      "kitamura\n",
      "durie\n",
      "penury\n",
      "bushell\n",
      "caput\n",
      "changchun\n",
      "durum\n",
      "missourians\n",
      "buri\n",
      "amanpour\n",
      "azura\n",
      "bourguignon\n",
      "fuld\n",
      "noblewoman\n",
      "pushup\n",
      "betelgeuse\n",
      "dachshunds\n",
      "gentlewoman\n",
      "letourneau\n",
      "pleurisy\n",
      "creutzfeldt\n",
      "pulliam\n",
      "detoured\n",
      "furukawa\n",
      "mercurio\n",
      "procurements\n",
      "panmure\n",
      "pluribus\n",
      "pocketful\n",
      "rurik\n",
      "ullrich\n",
      "curtice\n",
      "muro\n",
      "uhr\n",
      "baldur\n",
      "insurer's\n",
      "pneumatics\n",
      "bulloch\n",
      "fulcher\n",
      "bullfights\n",
      "dury\n",
      "kawamura\n",
      "europium\n",
      "matsumura\n",
      "coloratura\n",
      "curiouser\n",
      "fluorides\n",
      "puls\n"
     ]
    }
   ],
   "source": [
    "for word in simplified_cmudict:\n",
    "    if 'UH' in simplified_cmudict[word] and 'oo' not in word:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['K', 'UH', 'K']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_cmudict[\"cook\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t', 'ck']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(\"a|e|i|o|u\", \"tick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_vowel(letter:str):\n",
    "    return letter in VOWELS\n",
    "\n",
    "def is_vowel_sound(phoneme:str):\n",
    "    return phoneme in all_vowel_sounds\n",
    "\n",
    "def only_short_vowels(word:str):\n",
    "    # Extract the vowels and the vowel phonemes\n",
    "    word_vowels = [c for c in word if is_vowel(c)]\n",
    "    word_vowel_sounds = [phon for phon in simplified_cmudict[word] if phon in all_vowel_sounds]\n",
    "\n",
    "    # Check that the number of vowels is equal to the number of vowel sounds\n",
    "    if len(word_vowels) != len(word_vowel_sounds):\n",
    "        return False\n",
    "    \n",
    "    # Check that each vowel corresponds, in order, to (one of) its sounds\n",
    "    for i in range(len(word_vowels)):\n",
    "        if word_vowel_sounds[i] not in short_vowels[word_vowels[i]]:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "    \n",
    "\n",
    "def is_vc(word: str, allowed_blends_and_digraphs=None, include_long_vowels=False):\n",
    "    \"\"\"\n",
    "    Determines if a given word follows the \"VC\" (vowel-consonant) pattern.\n",
    "\n",
    "    A valid VC word must:\n",
    "    - Contain exactly two phonetic sounds: one vowel followed by one consonant.\n",
    "    - Only include short vowels unless `include_long_vowels` is set to True.\n",
    "    - Have a valid consonant ending, either a single consonant or one listed in `allowed_blends_and_digraphs`.\n",
    "\n",
    "    Parameters:\n",
    "        word (str): The word to check.\n",
    "        allowed_blends_and_digraphs (list, optional): A list of allowed consonant blends or digraphs. Defaults to an empty list.\n",
    "        include_long_vowels (bool, optional): Whether to allow long vowels. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the word follows the VC pattern, False otherwise.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the word contains more than one vowel.\n",
    "    \"\"\"\n",
    "    if allowed_blends_and_digraphs is None:\n",
    "        allowed_blends_and_digraphs = []\n",
    "\n",
    "    sounds = simplified_cmudict[word]\n",
    "\n",
    "    # Ensure the word has exactly two phonetic sounds: a vowel followed by a consonant\n",
    "    if len(sounds) != 2 or not is_vowel_sound(sounds[0]) or is_vowel_sound(sounds[1]):\n",
    "        return False\n",
    "\n",
    "    # If long vowels shouldn't be included, verify the vowel is short\n",
    "    if not include_long_vowels and not only_short_vowels(word):\n",
    "        return False\n",
    "\n",
    "    # Extract consonant part of the word\n",
    "    consonant_part = word[1:]\n",
    "\n",
    "    # Ensure no additional vowels exist beyond the first letter\n",
    "    if any(is_vowel(letter) for letter in consonant_part):\n",
    "        raise Exception(f\"The word '{word}' appears to have more than one vowel!\")\n",
    "\n",
    "    # Check if the consonant part is a single letter or an allowed blend/digraph\n",
    "    return len(consonant_part) == 1 or consonant_part in allowed_blends_and_digraphs\n",
    "    \n",
    "def is_cvc(word:str, allowed_blends_and_digraphs=None, include_long_vowels=False):\n",
    "    \"\"\"\n",
    "    Determines if a given word follows the \"CVC\" (consonant-vowel-consonant) pattern.\n",
    "\n",
    "    A valid CVC word must:\n",
    "    - Contain one consonant (or blend) followed by one vowel followed by one consonant (or blend).\n",
    "    - Only include short vowels unless `include_long_vowels` is set to True.\n",
    "    - Consonant sounds must be a single letter or one listed in `allowed_blends_and_digraphs`.\n",
    "\n",
    "    Parameters:\n",
    "        word (str): The word to check.\n",
    "        allowed_blends_and_digraphs (list, optional): A list of allowed consonant blends or digraphs. Defaults to an empty list.\n",
    "        include_long_vowels (bool, optional): Whether to allow long vowels. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the word follows the CVC pattern, False otherwise.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the word contains more than one vowel sound.\n",
    "    \"\"\"\n",
    "    if allowed_blends_and_digraphs is None:\n",
    "        allowed_blends_and_digraphs = []\n",
    "\n",
    "    sounds = simplified_cmudict[word]\n",
    "\n",
    "    # Ensure the word has exactly two phonetic sounds: a vowel followed by a consonant\n",
    "    if len(sounds) != 2 or not is_vowel_sound(sounds[0]) or is_vowel_sound(sounds[1]):\n",
    "        return False\n",
    "\n",
    "    # If long vowels shouldn't be included, verify the vowel is short\n",
    "    if not include_long_vowels and not only_short_vowels(word):\n",
    "        return False\n",
    "\n",
    "    # Extract consonant part of the word\n",
    "    consonant_part = word[1:]\n",
    "\n",
    "    # Ensure no additional vowels exist beyond the first letter\n",
    "    if any(is_vowel(letter) for letter in consonant_part):\n",
    "        raise Exception(f\"The word '{word}' appears to have more than one vowel!\")\n",
    "\n",
    "    # Check if the consonant part is a single letter or an allowed blend/digraph\n",
    "    return len(consonant_part) == 1 or consonant_part in allowed_blends_and_digraphs\n",
    "    return len(word) == 3 and not is_vowel(word[0]) and is_vowel(word[1]) and not is_vowel(word[2])\n",
    "\n",
    "def is_cvcc(word:str):\n",
    "    return\n",
    "def is_ccvc():\n",
    "    return\n",
    "def is_cvce(word:str):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bl-',\n",
       " 'br-',\n",
       " 'cl-',\n",
       " 'cr-',\n",
       " 'dr-',\n",
       " 'fl-',\n",
       " 'fr-',\n",
       " 'gh-',\n",
       " 'gl-',\n",
       " 'gn-',\n",
       " 'gr-',\n",
       " 'kn-',\n",
       " 'ph-',\n",
       " 'pl-',\n",
       " 'pr-',\n",
       " 'sc-',\n",
       " 'shr-',\n",
       " 'sk-',\n",
       " 'sl-',\n",
       " 'sm-',\n",
       " 'sn-',\n",
       " 'sp-',\n",
       " 'squ-',\n",
       " 'st-',\n",
       " 'sw-',\n",
       " 'tr-',\n",
       " 'wr-'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 5\n",
      "is 7\n",
      "it 11\n",
      "on 12\n",
      "as 17\n",
      "at 20\n",
      "an 29\n",
      "if 36\n",
      "up 43\n",
      "us 94\n",
      "am 159\n",
      "al 701\n",
      "ed 1690\n",
      "ad 1778\n",
      "im 2076\n",
      "et 2199\n",
      "un 2414\n",
      "id 2447\n",
      "em 2622\n",
      "el 2672\n",
      "en 2989\n",
      "op 3515\n",
      "il 4163\n",
      "um 5845\n",
      "os 6076\n",
      "ab 6428\n",
      "ip 6699\n",
      "oz 7036\n",
      "ag 7642\n",
      "es 7918\n",
      "og 10898\n",
      "ev 12041\n",
      "ul 14239\n",
      "ib 16045\n",
      "eb 20101\n",
      "ek 23254\n"
     ]
    }
   ],
   "source": [
    "for word in simplified_cmudict:\n",
    "    if is_vc(word):\n",
    "        print(word, top_n.index(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B', 'OW', 'T']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_cmudict['boat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_short_vowels(\"ought\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip out the emphasis bc it doesn't matter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
